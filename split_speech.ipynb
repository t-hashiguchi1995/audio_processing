{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://dev.classmethod.jp/articles/trial-python-webrtcvad/#toc-py-webrtcvad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import contextlib\n",
    "import sys\n",
    "import wave\n",
    "import pathlib\n",
    "\n",
    "import webrtcvad\n",
    "\n",
    "\n",
    "def read_audio(path):\n",
    "    \"\"\"MP3またはWAVファイルを読み込みます。\n",
    "\n",
    "    パスを受け取り、(PCMオーディオデータ, サンプルレート)を返します。\n",
    "    \"\"\"\n",
    "    import pydub\n",
    "\n",
    "    audio = pydub.AudioSegment.from_file(path)\n",
    "    \n",
    "    # モノラルに変換\n",
    "    if audio.channels > 1:\n",
    "        audio = audio.set_channels(1)\n",
    "    \n",
    "    # サンプルレートを確認\n",
    "    sample_rate = audio.frame_rate\n",
    "    assert sample_rate in (8000, 16000, 32000, 48000), f\"サポートされていないサンプルレート: {sample_rate}\"\n",
    "    \n",
    "    # PCMデータを取得\n",
    "    pcm_data = audio.raw_data\n",
    "    \n",
    "    return pcm_data, sample_rate\n",
    "\n",
    "def read_wave(path):\n",
    "    \"\"\"WAVファイルを読み込みます。\n",
    "\n",
    "    パスを受け取り、(PCMオーディオデータ, サンプルレート)を返します。\n",
    "    \"\"\"\n",
    "    with contextlib.closing(wave.open(path, 'rb')) as wf:\n",
    "        num_channels = wf.getnchannels()\n",
    "        assert num_channels == 1\n",
    "        sample_width = wf.getsampwidth()\n",
    "        assert sample_width == 2\n",
    "        sample_rate = wf.getframerate()\n",
    "        assert sample_rate in (8000, 16000, 32000, 48000)\n",
    "        pcm_data = wf.readframes(wf.getnframes())\n",
    "        return pcm_data, sample_rate\n",
    "\n",
    "def write_wave(path, audio, sample_rate):\n",
    "    \"\"\"WAVファイルを書き込みます。\n",
    "\n",
    "    パス、PCMオーディオデータ、サンプルレートを受け取ります。\n",
    "    \"\"\"\n",
    "    with contextlib.closing(wave.open(path, 'wb')) as wf:\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(2)\n",
    "        wf.setframerate(sample_rate)\n",
    "        wf.writeframes(audio)\n",
    "\n",
    "\n",
    "class Frame(object):\n",
    "    \"\"\"オーディオデータの「フレーム」を表現するクラス\"\"\"\n",
    "    def __init__(self, bytes, timestamp, duration):\n",
    "        self.bytes = bytes\n",
    "        self.timestamp = timestamp\n",
    "        self.duration = duration\n",
    "\n",
    "\n",
    "def frame_generator(frame_duration_ms, audio, sample_rate):\n",
    "    \"\"\"PCMオーディオデータからオーディオフレームを生成します。\n",
    "\n",
    "    フレーム長（ミリ秒）、PCMデータ、サンプルレートを受け取り、\n",
    "    指定された長さのフレームを生成します。\n",
    "    \"\"\"\n",
    "    n = int(sample_rate * (frame_duration_ms / 1000.0) * 2)\n",
    "    offset = 0\n",
    "    timestamp = 0.0\n",
    "    duration = (float(n) / sample_rate) / 2.0\n",
    "    while offset + n < len(audio):\n",
    "        yield Frame(audio[offset:offset + n], timestamp, duration)\n",
    "        timestamp += duration\n",
    "        offset += n\n",
    "\n",
    "\n",
    "def vad_collector(sample_rate: int, frame_duration_ms: int,\n",
    "    padding_duration_ms: int, vad: webrtcvad.Vad, frames: list[Frame],\n",
    "    voice_trigger_on_thres: float=0.9, voice_trigger_off_thres: float=0.1) -> list[dict]:\n",
    "    \"\"\"音声非音声セグメント処理\n",
    "\n",
    "    Args:\n",
    "        sample_rate (int): 単位時間あたりのサンプル数[Hz]\n",
    "        frame_duration_ms (int): フレーム長\n",
    "        padding_duration_ms (int): ガード長\n",
    "        vad (webrtcvad.Vad): VADオブジェクト\n",
    "        frames (list[Frame]): フレーム分割された音声データ\n",
    "        voice_trigger_on_thres (float, optional): 音声セグメント開始と判断する閾値。デフォルト値は0.9\n",
    "        voice_trigger_off_thres (float, optional): 音声セグメント終了と判断する閾値。デフォルト値は0.1\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: セグメント結果\n",
    "    \"\"\"\n",
    "    # ガードするフレーム数\n",
    "    num_padding_frames = int(padding_duration_ms / frame_duration_ms)\n",
    "\n",
    "    # フレームバッファ\n",
    "    frame_buffer = []\n",
    "\n",
    "    # 音声検出トリガーの状態\n",
    "    triggered = False\n",
    "\n",
    "    voiced_frames = []\n",
    "    vu_segments = []\n",
    "\n",
    "    for frame in frames:\n",
    "        is_speech = vad.is_speech(frame.bytes, sample_rate)\n",
    "        frame_buffer.append((frame, is_speech))\n",
    "\n",
    "        # 非音声セグメントの処理\n",
    "        if not triggered:\n",
    "            # 過去フレームの音声判定数を計算\n",
    "            num_voiced = len([f for f, speech in frame_buffer[-num_padding_frames:] if speech])\n",
    "\n",
    "            # 音声セグメント開始の判定\n",
    "            if num_voiced > voice_trigger_on_thres * num_padding_frames:\n",
    "                triggered = True\n",
    "\n",
    "                # 非音声セグメントの保存\n",
    "                audio_data = b''.join([f.bytes for f, _ in frame_buffer[:-num_padding_frames]])\n",
    "                vu_segments.append({\"vad\": 0, \"audio_size\": len(audio_data), \"audio_data\": audio_data})\n",
    "\n",
    "                # 音声フレームの保持\n",
    "                for f, _ in frame_buffer[-num_padding_frames:]:\n",
    "                    voiced_frames.append(f)\n",
    "                frame_buffer = []\n",
    "\n",
    "        # 音声セグメントの処理\n",
    "        else:\n",
    "            voiced_frames.append(frame)\n",
    "\n",
    "            # 過去フレームの非音声判定数を計算\n",
    "            num_unvoiced = len([f for f, speech in frame_buffer[-num_padding_frames:] if not speech])\n",
    "\n",
    "            # 音声セグメント終了の判定\n",
    "            if num_unvoiced > (1 - voice_trigger_off_thres) * num_padding_frames:\n",
    "                triggered = False\n",
    "\n",
    "                # 音声セグメントの保存\n",
    "                audio_data = b''.join([f.bytes for f in voiced_frames])\n",
    "                vu_segments.append({\"vad\": 1, \"audio_size\": len(audio_data), \"audio_data\": audio_data})\n",
    "                voiced_frames = []\n",
    "\n",
    "                frame_buffer = []\n",
    "\n",
    "    # 最終セグメントの処理\n",
    "    if triggered:\n",
    "        audio_data = b''.join([f.bytes for f in voiced_frames])\n",
    "        vu_segments.append({\"vad\": 1, \"audio_size\": len(audio_data), \"audio_data\": audio_data})\n",
    "    else:\n",
    "        audio_data = b''.join([f.bytes for f, _ in frame_buffer])\n",
    "        vu_segments.append({\"vad\": 0, \"audio_size\": len(audio_data), \"audio_data\": audio_data})\n",
    "\n",
    "    return vu_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_segments(folder_path, file_name):\n",
    "    # wav読込\n",
    "    audio_data, sample_rate = read_audio(folder_path + file_name)\n",
    "\n",
    "    # VADクラス\n",
    "    vad = webrtcvad.Vad(0)\n",
    "\n",
    "    # フレーム分割\n",
    "    frames = frame_generator(30, audio_data, sample_rate)\n",
    "    frames = list(frames)\n",
    "\n",
    "    # セグメント結果\n",
    "    # vu_segments = vad_collector(sample_rate, 30, 300, vad, frames)\n",
    "    vu_segments = vad_collector(sample_rate, 30, 300, vad, frames, voice_trigger_off_thres=0.8)\n",
    "\n",
    "\n",
    "    # 初期化\n",
    "    vu_segments_merged = []\n",
    "    count = 0\n",
    "    while count < len(vu_segments):\n",
    "        s = vu_segments[count].copy()\n",
    "\n",
    "        # 5秒以下なら次のセグメントとマージ\n",
    "        while s[\"audio_size\"] < 5 * 2 * sample_rate:\n",
    "            # 次のセグメントがない場合は強制終了\n",
    "            if count == len(vu_segments) - 1:\n",
    "                break\n",
    "\n",
    "            # マージ処理\n",
    "            s[\"audio_size\"] = s[\"audio_size\"] + vu_segments[count+1][\"audio_size\"]\n",
    "            s[\"audio_data\"] = s[\"audio_data\"] + vu_segments[count+1][\"audio_data\"]\n",
    "            count += 1\n",
    "\n",
    "        # マージされたセグメントを格納\n",
    "        vu_segments_merged.append(s)\n",
    "        count += 1\n",
    "\n",
    "    # wavファイル格納先作成\n",
    "    segments_dir = pathlib.Path(\"./segments_merged/\")\n",
    "    segments_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # レコード作成しつつwavファイル出力\n",
    "    for_df = []\n",
    "    for i, segment in enumerate(vu_segments_merged):\n",
    "        path = segments_dir.joinpath(f\"{file_name.split('.')[0]}_segment-{i:03d}.wav\")\n",
    "        write_wave(str(path), segment['audio_data'], sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for file in os.listdir(\"./mp3/\"):\n",
    "    try:\n",
    "        split_segments(\"./mp3/\", file)\n",
    "    except:\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(os.listdir(\"./segments_merged/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
